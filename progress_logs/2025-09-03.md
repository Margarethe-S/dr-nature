## ğŸ§  Progress Log â€“ 03.09.2025


## ğŸ”¬ Overnight Test & Erkenntnisse

- **Overnight-Test des Hauptprompts (~900 Tokens):**
  - Prompt wurde gestern Abend gestartet und lief durchgehend Ã¼ber Nacht.
  - CPU und RAM zeigten kontinuierlich AktivitÃ¤t.
  - Kein Fehler oder Abbruch erkennbar, jedoch keine RÃ¼ckmeldung vom Modell â€“ Status blieb bei â€Generatingâ€œ.
  - Dauer bis zur PrÃ¼fung: ca. 22 Stunden.
  - Erkenntnis: CPU-basiertes Inference dauert bei komplexen Prompts potenziell extrem lang. Keine Garantie auf Antwortzeit.
  - Test wird als abgeschlossen dokumentiert, Output bleibt offen.
  - Geplante Wiederholung nur bei spÃ¤terem Hardware-Upgrade (GPU-Nutzung).

## ğŸ”§ Technische Reflexion

- Modell: **EM-German-Mistral-V01** (lokal, LM Studio)
- Keine AbstÃ¼rze, System blieb stabil, Ressourcen wurden weiter beansprucht.
- Abbruch erfolgt bewusst manuell, um Ressourcen freizugeben und nÃ¤chste Entwicklungsphasen zu ermÃ¶glichen.
- Bewertung: **Test erfolgreich im Sinne der Erkenntnisgewinnung.**

## ğŸ§‘â€ğŸ« Lehrerbesprechung
- Nach ausfÃ¼hrlicher RÃ¼cksprache mit dem Lehrer wurde folgender Hinweis gegeben:
  - Eventuell bietet **LM Studio** erweiterte Optionen zur Leistungssteigerung Ã¼ber manuelle Konfiguration (RAM-/CPU-Zuweisung).
  - Alternativ wÃ¤re ein **Testlauf Ã¼ber AWS** denkbar, um hÃ¶here RechenkapazitÃ¤ten zur VerfÃ¼gung zu stellen (z.â€¯B. EC2 mit GPU).
  - Diese Optionen werden geprÃ¼ft und in die Planung aufgenommen.

### ğŸ”„ Branch-Synchronisierung abgeschlossen (03.09.2025)

- Alle lokalen Branches (`main`, `docs/10-dokumentation-struktur`, `test/09-lm-api-and-prompt-tuning`) wurden mit dem aktuellen Stand aus `main` gemerged.
- Ã„nderungen wurden erfolgreich mit `git push origin` in die jeweiligen Remote-Branches Ã¼bertragen.
- Hinweis: Arbeiten erfolgt weiterhin ausschlieÃŸlich auf lokalen Branches â€“ niemals auf `origin/...`.

âœ… Projektstruktur ist nun vollstÃ¤ndig synchronisiert.

## âœ… Funktionierende Prompts (inkl. Ladezeit)

| Prompt-Datei           | Ergebnis       | Ladezeit     |
|------------------------|----------------|--------------|
| prompttest.txt         | funktioniert   | ca. 2:20 Min |
| prompt_test_1.2.txt    | funktioniert   | ca. 2:00 Min |

## âŒ Nicht funktionierende Prompts

| Prompt-Datei           | Ergebnis                | Ladezeit         |
|------------------------|-------------------------|------------------|
| prompt_test_1.1.txt    | lÃ¤dt unendlich / Fehler | >10 Min, abgebrochen |
| prompt_test_1.3.txt    | lÃ¤dt unendlich / Fehler | >10 Min, abgebrochen |
| prompt_test_1.4.txt    | lÃ¤dt unendlich / Fehler | >10 Min, abgebrochen |

## ğŸ”§ Technische Anpassungen

- Modus auf **Balanced** gestellt in LM Studio
- RAM-Nutzung steigt auf ca. **6â€¯GB**, CPU zwischen **1â€“6â€¯%**
- **Zeitmessung** in Testdatei integriert zur Performanceanalyse
- Verschiedene Prompts erstellt und nacheinander getestet

## ğŸ“Œ Erkenntnisse

- **Sonderzeichen, Leerzeichen und Promptstruktur** beeinflussen Ladezeit deutlich
- Terminal- und GUI-Antworten variieren sinnvoll â†’ **verschiedene AntwortmÃ¶glichkeiten kÃ¶nnen getestet werden**

## ğŸ“š Recherchen & Hintergrund

- ZukunftsÃ¼berlegung: **ggf. AWS oder Azure** fÃ¼r stabileres Cloud-Testing (nur optional, Projekt bleibt lokal)
- Promptvarianten abgeglichen, **unterschiedliche Reaktionen beobachtet**
- **Zeiterfassung eingebaut**
- GUI und Terminal **direkt verglichen**
- Log aktualisiert, **Tests dokumentiert**

---

âœ… **Status**: *Tag abgeschlossen mit wertvollem Erkenntnisgewinn zur Inferenz-Zeit und Ressourcen-Nutzung bei CPU-only Setup.*
