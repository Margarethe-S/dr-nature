## 🧠 Progress Log – 03.09.2025


## 🔬 Overnight Test & Erkenntnisse

- **Overnight-Test des Hauptprompts (~900 Tokens):**
  - Prompt wurde gestern Abend gestartet und lief durchgehend über Nacht.
  - CPU und RAM zeigten kontinuierlich Aktivität.
  - Kein Fehler oder Abbruch erkennbar, jedoch keine Rückmeldung vom Modell – Status blieb bei „Generating“.
  - Dauer bis zur Prüfung: ca. 22 Stunden.
  - Erkenntnis: CPU-basiertes Inference dauert bei komplexen Prompts potenziell extrem lang. Keine Garantie auf Antwortzeit.
  - Test wird als abgeschlossen dokumentiert, Output bleibt offen.
  - Geplante Wiederholung nur bei späterem Hardware-Upgrade (GPU-Nutzung).

## 🔧 Technische Reflexion

- Modell: **EM-German-Mistral-V01** (lokal, LM Studio)
- Keine Abstürze, System blieb stabil, Ressourcen wurden weiter beansprucht.
- Abbruch erfolgt bewusst manuell, um Ressourcen freizugeben und nächste Entwicklungsphasen zu ermöglichen.
- Bewertung: **Test erfolgreich im Sinne der Erkenntnisgewinnung.**

## 🧑‍🏫 Lehrerbesprechung
- Nach ausführlicher Rücksprache mit dem Lehrer wurde folgender Hinweis gegeben:
  - Eventuell bietet **LM Studio** erweiterte Optionen zur Leistungssteigerung über manuelle Konfiguration (RAM-/CPU-Zuweisung).
  - Alternativ wäre ein **Testlauf über AWS** denkbar, um höhere Rechenkapazitäten zur Verfügung zu stellen (z. B. EC2 mit GPU).
  - Diese Optionen werden geprüft und in die Planung aufgenommen.

### 🔄 Branch-Synchronisierung abgeschlossen (03.09.2025)

- Alle lokalen Branches (`main`, `docs/10-dokumentation-struktur`, `test/09-lm-api-and-prompt-tuning`) wurden mit dem aktuellen Stand aus `main` gemerged.
- Änderungen wurden erfolgreich mit `git push origin` in die jeweiligen Remote-Branches übertragen.
- Hinweis: Arbeiten erfolgt weiterhin ausschließlich auf lokalen Branches – niemals auf `origin/...`.

✅ Projektstruktur ist nun vollständig synchronisiert.

## ✅ Funktionierende Prompts (inkl. Ladezeit)

| Prompt-Datei           | Ergebnis       | Ladezeit     |
|------------------------|----------------|--------------|
| prompttest.txt         | funktioniert   | ca. 2:20 Min |
| prompt_test_1.2.txt    | funktioniert   | ca. 2:00 Min |

## ❌ Nicht funktionierende Prompts

| Prompt-Datei           | Ergebnis                | Ladezeit         |
|------------------------|-------------------------|------------------|
| prompt_test_1.1.txt    | lädt unendlich / Fehler | >10 Min, abgebrochen |
| prompt_test_1.3.txt    | lädt unendlich / Fehler | >10 Min, abgebrochen |
| prompt_test_1.4.txt    | lädt unendlich / Fehler | >10 Min, abgebrochen |

## 🔧 Technische Anpassungen

- Modus auf **Balanced** gestellt in LM Studio
- RAM-Nutzung steigt auf ca. **6 GB**, CPU zwischen **1–6 %**
- **Zeitmessung** in Testdatei integriert zur Performanceanalyse
- Verschiedene Prompts erstellt und nacheinander getestet

## 📌 Erkenntnisse

- **Sonderzeichen, Leerzeichen und Promptstruktur** beeinflussen Ladezeit deutlich
- Terminal- und GUI-Antworten variieren sinnvoll → **verschiedene Antwortmöglichkeiten können getestet werden**

## 📚 Recherchen & Hintergrund

- Zukunftsüberlegung: **ggf. AWS oder Azure** für stabileres Cloud-Testing (nur optional, Projekt bleibt lokal)
- Promptvarianten abgeglichen, **unterschiedliche Reaktionen beobachtet**
- **Zeiterfassung eingebaut**
- GUI und Terminal **direkt verglichen**
- Log aktualisiert, **Tests dokumentiert**

---

✅ **Status**: *Tag abgeschlossen mit wertvollem Erkenntnisgewinn zur Inferenz-Zeit und Ressourcen-Nutzung bei CPU-only Setup.*
